# â˜ï¸ Cloud S3 ETL Pipeline: YAML-Driven Sync & Download Framework

![Build Status](https://img.shields.io/badge/build-passing-brightgreen)
![Python](https://img.shields.io/badge/python-3.9+-blue)
![AWS S3](https://img.shields.io/badge/AWS-S3-orange)
![Power BI Ready](https://img.shields.io/badge/PowerBI-ready-yellowgreen)
![License](https://img.shields.io/badge/license-MIT-lightgrey)

> ğŸ”„ A production-grade, YAML-configurable pipeline to automate secure downloads from AWS S3 using Python, with sync tracking for incremental updates and integration-ready outputs for tools like Power BI, Pandas, or SQL engines.

---

## ğŸš€ Why This Project?

Companies often need to automate and monitor recurring data pulls from cloud storage (S3). Manual effort is time-consuming and error-prone. This ETL pipeline:

- âœ… Automates end-to-end download from S3
- âœ… Tracks sync status via Parquet snapshot
- âœ… Supports **incremental** or **full refresh** modes
- âœ… Enables downstream use in Power BI, SQL, Pandas
- âœ… Is cloud-ready, secure, and highly extensible

---

## ğŸ§  Real-World Use Case

> **Domain:** Healthcare, Operations, Government M&E  
> **Scenario:** Download daily patient reports or IoT logs from S3, auto-clean them, and feed into Power BI dashboards without duplicating downloads or manual triggers.

---

## ğŸ”§ Technologies Used

| Stack Area     | Tools / Libraries                         |
|----------------|-------------------------------------------|
| ğŸ Language     | Python 3.9+                               |
| â˜ï¸ Cloud        | AWS S3 (Boto3)                            |
| ğŸ” Security     | `.env`-based IAM key handling             |
| âš™ï¸ Config       | YAML-driven, modular structure            |
| ğŸ“Š Output Ready | Power BI â€¢ Pandas â€¢ SQL â€¢ Excel           |
| ğŸ§ª Tracking     | Parquet sync-state (incremental updates)  |
| ğŸ“¦ Packaging    | Virtualenv compatible                     |
| â±ï¸ Performance  | Multi-threaded downloads (via `tqdm`)     |

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ config_loader.py # Loads YAML config
â”‚ â”œâ”€â”€ file_utils.py # File system helpers, sync state
â”‚ â”œâ”€â”€ s3_manager.py # S3 logic (connect, list, download)
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ default_config.yaml # Bucket/prefix/filter settings
â”‚ â”œâ”€â”€ config.env # AWS credentials
â”œâ”€â”€ s3_downloader_pipeline.py # Main runner script
â””â”€â”€ README.md

---

## âš™ï¸ How It Works

1. âœ… **Load credentials** from `.env`
2. ğŸ“‘ **Read config** from `default_config.yaml`
3. ğŸ” **Connect securely** to AWS S3 via Boto3
4. ğŸ“¦ **List & filter objects** (by extension, name, timestamp)
5. â¬‡ï¸ **Download files** with `ThreadPoolExecutor`
6. ğŸ“Š **Save sync state** in `Parquet` for incremental runs

---

## ğŸ“„ Configuration Examples

### `config.env`
AWS_ACCESS_KEY_ID=AKIxxxxxxxxxxxx
AWS_SECRET_ACCESS_KEY=xxxxxxx
AWS_REGION=ap-south-1

### `default_config.yaml`
```yaml
s3:
  your-bucket-name:
    - logs/
    - uploads/

filters:
  include_extensions: ['.csv', '.json']
  exclude_files: ['ignore_this.csv']

sync:
  loc_download: ./downloads
  mode: incremental  # or full_refresh
  
  # Install requirements
pip install -r requirements.txt

# Run the ETL pipeline
python s3_downloader_pipeline.py

ğŸ“ˆ Example Output
Files downloaded to: /downloads

Metadata tracked in: /downloads/sync_state.parquet

Dashboard-ready format: .csv, .json, .parquet

ğŸ§ª Future Enhancements
 Airflow / Prefect integration

 Scheduling via GitHub Actions or cron

 File validation module (schema + data profiling)

 Support for other cloud providers (GCS, Azure)

ğŸ§‘â€ğŸ’¼ Ideal For
Data Analysts automating data ingestion

Data Engineers building ETL pipelines

BI Developers feeding cloud data into dashboards

Healthcare & GovTech projects requiring audit-ready downloads

ğŸ“¬ Contact
Ashutosh Singh
ğŸ“§ ashutoshsinghindore@gmail.com
ğŸ”— LinkedIn
ğŸ™ GitHub