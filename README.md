# â˜ï¸ Cloud S3 ETL Pipeline: YAML-Driven Sync & Download Framework

![Python](https://img.shields.io/badge/python-3.9+-blue)
![AWS S3](https://img.shields.io/badge/AWS-S3-orange)
![Power BI Ready](https://img.shields.io/badge/PowerBI-ready-yellowgreen)
![ETL](https://img.shields.io/badge/ETL-Cloud%20Pipeline-brightgreen)
![License](https://img.shields.io/badge/license-MIT-lightgrey)

> ğŸ”„ A production-grade, YAML-configurable pipeline to automate secure downloads from AWS S3 using Python, with sync tracking for incremental updates and integration-ready outputs for tools like Power BI, Pandas, or SQL engines.

---

## ğŸš€ Why This Project?

Companies often need to automate and monitor recurring data pulls from cloud storage (S3). Manual effort is time-consuming and error-prone. This ETL pipeline:

- âœ… Automates end-to-end download from S3
- âœ… Tracks sync status via Parquet snapshot
- âœ… Supports **incremental** or **full refresh** modes
- âœ… Enables downstream use in Power BI, SQL, Pandas
- âœ… Is cloud-ready, secure, and highly extensible

---

## ğŸ§  Real-World Use Case

> **Domain:** Healthcare, Operations, Government M&E  
> **Scenario:** Download daily patient reports or IoT logs from S3, auto-clean them, and feed into Power BI dashboards without duplicating downloads or manual triggers.

---

## ğŸ”§ Technologies Used

| Stack Area     | Tools / Libraries                         |
|----------------|-------------------------------------------|
| ğŸ Language     | Python 3.9+                               |
| â˜ï¸ Cloud        | AWS S3 (Boto3)                            |
| ğŸ” Security     | `.env`-based IAM key handling             |
| âš™ï¸ Config       | YAML-driven, modular structure            |
| ğŸ“Š Output Ready | Power BI â€¢ Pandas â€¢ SQL â€¢ Excel           |
| ğŸ§ª Tracking     | Parquet sync-state (incremental updates)  |
| ğŸ“¦ Packaging    | Virtualenv compatible                     |
| â±ï¸ Performance  | Multi-threaded downloads (via `tqdm`)     |

---

## ğŸ“ Project Structure
```
Cloud-ETL-S3-Pipeline/
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ config.env # ğŸ” AWS credentials (use with dotenv)
â”‚ â””â”€â”€ default_config.yaml # âš™ï¸ YAML-based S3 bucket and sync settings
â”‚ â””â”€â”€ config.env.txt # Dummy config.env file
â”‚
â”œâ”€â”€ downloads/ # ğŸ“ Local download directory (auto-created)
â”‚ â””â”€â”€ sync_state.parquet # ğŸ§  Parquet file to track incremental syncs
â”‚
â”œâ”€â”€ src/ # ğŸ§  Core logic modules
â”‚ â”œâ”€â”€ config_loader.py # ğŸ—‚ï¸ Reads and parses YAML config
â”‚ â”œâ”€â”€ file_utils.py # ğŸ“„ File discovery, sync-state writing
â”‚ â””â”€â”€ s3_manager.py # â˜ï¸ S3 connection, filtering, multi-threaded download
â”‚
â”œâ”€â”€ s3_downloader_pipeline.py # ğŸš€ Main entry script to execute pipeline
â”œâ”€â”€ requirements.txt # ğŸ“¦ Project dependencies
â”œâ”€â”€ README.md # ğŸ“˜ This file
â””â”€â”€ .gitignore # ğŸš« Ignore credentials, downloads, cache
```
---

## âš™ï¸ How It Works

1. âœ… **Load credentials** from `src/config.env`
2. ğŸ“‘ **Read config** from `default_config.yaml`
3. ğŸ” **Connect securely** to AWS S3 via Boto3
4. ğŸ“¦ **List & filter objects** (by extension, name, timestamp)
5. â¬‡ï¸ **Download files** with `ThreadPoolExecutor`
6. ğŸ“Š **Save sync state** in `Parquet` for incremental runs

---

## ğŸ“„ Configuration Examples

### `config.env`
```yaml
AWS_ACCESS_KEY_ID=AKIxxxxxxxxxxxx
AWS_SECRET_ACCESS_KEY=xxxxxxx
AWS_REGION=ap-south-1
```

### `default_config.yaml`
```yaml
# ğŸ“¦ S3 Configuration
s3:
  prod-completed:                # (REQUIRED) S3 bucket name (e.g., healthcare-data-bucket)
    - 2025-05-30/                # Prefix/folder inside bucket. Must end with '/'.
    - 2025-03-01/                # Add more prefixes as needed.

# ğŸ”„ Sync Mode Configuration
sync:
  loc_download: ../downloads     # (REQUIRED) Local directory to save downloaded files.
  mode: incremental              # Options:
                                 #   - full_refresh: Download all files
                                 #   - incremental: Only new/updated files
                                 #   - mirror: Match S3 exactly (delete local extras)
  threads: 8                     # Number of parallel download threads (default: 12)

# ğŸ¯ File Filters
filters:
  include_extensions:            # Only download files with these extensions
    - .mp4
    - .xlsx
  exclude_files:                 # Skip these exact filenames (optional)
    - skip_this_file.csv
```

```bash
# Install requirements
pip install -r requirements.txt

# Run the ETL pipeline
python s3_downloader_pipeline.py
```

## ğŸ§ª Future Enhancements
- Integration with Airflow or Prefect for orchestration  
- Scheduling support via GitHub Actions or cron  
- File validation module with schema checks and data profiling  
- Compatibility with additional cloud providers (GCS, Azure)

## ğŸ§‘ğŸ’¼ Ideal For
- **Data Analysts** automating ingestion from S3  
- **Data Engineers** building scalable ETL pipelines  
- **BI Developers** integrating cloud data into dashboards  
- **Healthcare & GovTech teams** needing audit-ready data downloads

## ğŸ“¬ Contact

**Ashutosh Singh**  
ğŸ“§ [ashutoshsinghindore@gmail.com](mailto:ashutoshsinghindore@gmail.com)  
ğŸ”— [LinkedIn](https://linkedin.com/in/ashutoshsinghindore)  
ğŸ™ [GitHub](https://github.com/AshutoshsinghIndore)
